{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 人工智能课程25-作业一-垃圾短信分类"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1.实验介绍"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.1 实验背景\n",
    "垃圾短信 (Spam Messages，SM) 是指未经过用户同意向用户发送不愿接收的商业广告或者不符合法律规范的短信。    \n",
    "随着手机的普及，垃圾短信在日常生活日益泛滥，已经严重的影响到了人们的正常生活娱乐，乃至社会的稳定。     \n",
    "据 360 公司 2020 年第一季度有关手机安全的报告提到，360 手机卫士在第一季度共拦截各类垃圾短信约 34.4 亿条，平均每日拦截垃圾短信约 3784.7 万条。      \n",
    "大数据时代的到来使得大量个人信息数据得以沉淀和积累，但是庞大的数据量缺乏有效的整理规范；   \n",
    "在面对量级如此巨大的短信数据时，为了保证更良好的用户体验，如何从数据中挖掘出更多有意义的信息为人们免受垃圾短信骚扰成为当前亟待解决的问题。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.2 实验要求\n",
    "1) 任务提供包括数据读取、基础模型、模型训练等基本代码  \n",
    "2) 需完成核心模型构建代码，并将模型在测试数据集上的预测结果下载到本地。  \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.3 实验环境 \n",
    "可以使用基于 Python 的 Pandas、Numpy、Sklearn 等库进行相关特征处理，使用 Paddle 框架训练分类器，使用过程中请注意 Python 包（库）的版本。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.4 作业提交\n",
    "1. 将实验二和实验三Todo部分的代码、实验四的完整代码放到todo.py文件进行提交。\n",
    "2. 将最终在私有测试集上的预测结果'predictions.csv'，下载保存到本地文件predictions.csv，进行提交。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.5 参考资料\n",
    "- Numpy：https://www.numpy.org/\n",
    "- Pandas: https://pandas.pydata.org/\n",
    "- Sklearn: https://scikit-learn.org/\n",
    "- jieba: https://github.com/fxsjy/jieba\n",
    "- 四川大学机器智能实验室停用词库：https://github.com/goto456/stopwords/blob/master/scu_stopwords.txt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "toc-hr-collapsed": false
   },
   "source": [
    "# 2.实验内容"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.1 数据集\n",
    "- 该数据集包括了约 1 万条数据，有 3 个字段 label、 message 和 msg_new， 分别代表了短信的类别、短信的内容和分词后的短信\n",
    "- 中文分词工具 [jieba](https://github.com/fxsjy/jieba)\n",
    "- 0 代表正常的短信，1 代表恶意的短信\n",
    "- 正常短信和恶意短信举例：\n",
    "\n",
    "|label|message（短信内容）|msg_new（短信分词后）|\n",
    "|--|--|--|\n",
    "|0|人们经常是失去了才发现它的珍贵|人们 经常 是 失去 了 才 发现 它 的 珍贵|\n",
    "|1|本人现在承办驾驶证业务!招收学员，一对 一教学|本人 现在 承办 驾驶证 业务 ! 招收 学员 ， 一对   一 教学|"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# 导入相关的包\n",
    "import paddle\n",
    "import paddle.nn.functional as F\n",
    "import numpy as np\n",
    "import os\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "os.environ[\"HDF5_USE_FILE_LOCKING\"] = \"FALSE\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.status.busy": "2025-03-21T07:54:20.550005Z",
     "iopub.status.idle": "2025-03-21T07:54:20.550334Z",
     "shell.execute_reply": "2025-03-21T07:54:20.550164Z",
     "shell.execute_reply.started": "2025-03-21T07:54:20.550151Z"
    },
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "# 解压数据集\n",
    "import os\n",
    "import zipfile\n",
    "\n",
    "if not os.path.exists(\"./sms_v4\"):\n",
    "    zf = zipfile.ZipFile('./sms_v4.zip','r')\n",
    "    # Extract a member from the archive to the current working directory\n",
    "    for f in zf.namelist():\n",
    "        zf.extract(f, './data/sms_v4')  # 循环解压，将文件解压到指定路径\n",
    "    zf.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.status.busy": "2025-03-21T07:54:20.551521Z",
     "iopub.status.idle": "2025-03-21T07:54:20.551873Z",
     "shell.execute_reply": "2025-03-21T07:54:20.551719Z",
     "shell.execute_reply.started": "2025-03-21T07:54:20.551705Z"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# 数据集的路径\n",
    "data_path = \"./data/sms_v4/sms_pub.csv\"\n",
    "# 读取数据\n",
    "sms = pd.read_csv(data_path, encoding='utf-8')\n",
    "# 显示前 5 条数据\n",
    "sms.head()\n",
    "# 私有测试集的路径\n",
    "data_path = \"./data/sms_v4/sms_private.csv\"\n",
    "# 读取私有测试集数据\n",
    "sms_private = pd.read_csv(data_path, encoding='utf-8')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.status.busy": "2025-03-21T07:54:20.552846Z",
     "iopub.status.idle": "2025-03-21T07:54:20.553456Z",
     "shell.execute_reply": "2025-03-21T07:54:20.553300Z",
     "shell.execute_reply.started": "2025-03-21T07:54:20.553286Z"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# 显示数据集的一些信息\n",
    "sms.groupby('label').describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.2 数据处理"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2.1 停用词"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "停用词是指在信息检索中，为节省存储空间和提高搜索效率，在处理自然语言数据（或文本）之前或之后会自动过滤掉某些字或词，这些字或词即被称为 Stop Words（停用词）。      \n",
    "这些停用词都是人工输入、非自动化生成的，生成后的停用词会形成一个停用词库。        \n",
    "本次作业中采用的是[四川大学机器智能实验室停用词库](https://github.com/goto456/stopwords/blob/master/scu_stopwords.txt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.status.busy": "2025-03-21T07:54:20.554265Z",
     "iopub.status.idle": "2025-03-21T07:54:20.554846Z",
     "shell.execute_reply": "2025-03-21T07:54:20.554691Z",
     "shell.execute_reply.started": "2025-03-21T07:54:20.554676Z"
    },
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "def read_stopwords(stopwords_path):\n",
    "    \"\"\"\n",
    "    读取停用词库\n",
    "    :param stopwords_path: 停用词库的路径\n",
    "    :return: 停用词列表\n",
    "    \"\"\"\n",
    "    with open(stopwords_path, 'r', encoding='utf-8') as f:\n",
    "        stopwords = f.read()\n",
    "    stopwords = stopwords.splitlines()\n",
    "    return stopwords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.status.busy": "2025-03-21T07:54:20.555790Z",
     "iopub.status.idle": "2025-03-21T07:54:20.556227Z",
     "shell.execute_reply": "2025-03-21T07:54:20.556096Z",
     "shell.execute_reply.started": "2025-03-21T07:54:20.556082Z"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# 停用词库路径\n",
    "stopwords_path = r'./data/sms_v4/scu_stopwords.txt'\n",
    "# 读取停用词\n",
    "stopwords = read_stopwords(stopwords_path)\n",
    "# 展示一些停用词\n",
    "print(stopwords[-20:])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2.2 文本向量化的方法"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**1. CountVectorizer**  \n",
    "目前拥有的数据是长度不统一的文本数据，而绝大多数机器学习算法需要的输入是向量，因此文本类型的数据需要经过处理得到向量。    \n",
    "我们可以借助 sklearn 中 **CountVectorizer** 来实现文本的向量化，CountVectorizer 实际上是在统计**每个词出现的次数**，这样的模型也叫做**词袋模型**。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.status.busy": "2025-03-21T07:54:20.557461Z",
     "iopub.status.idle": "2025-03-21T07:54:20.558042Z",
     "shell.execute_reply": "2025-03-21T07:54:20.557882Z",
     "shell.execute_reply.started": "2025-03-21T07:54:20.557853Z"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# 假如我们有这样三条短信\n",
    "simple_train = ['call you tonight', 'Call me a cab', 'Please call me... PLEASE!']\n",
    "\n",
    "# 导入 CountVectorizer\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "vect = CountVectorizer()\n",
    "\n",
    "# 从训练数据中学习词汇表\n",
    "vect.fit(simple_train)\n",
    "\n",
    "# 查看学习到的词汇表\n",
    "vect.get_feature_names_out()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.status.busy": "2025-03-21T07:54:20.558970Z",
     "iopub.status.idle": "2025-03-21T07:54:20.559390Z",
     "shell.execute_reply": "2025-03-21T07:54:20.559259Z",
     "shell.execute_reply.started": "2025-03-21T07:54:20.559246Z"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# 将训练数据向量化，得到一个矩阵\n",
    "simple_train_dtm = vect.transform(simple_train)\n",
    "# 由于该矩阵的维度可能十分大，而其中大部分都为 0，所以会采用稀疏矩阵来存储\n",
    "simple_train_dtm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.status.busy": "2025-03-21T07:54:20.560316Z",
     "iopub.status.idle": "2025-03-21T07:54:20.560586Z",
     "shell.execute_reply": "2025-03-21T07:54:20.560461Z",
     "shell.execute_reply.started": "2025-03-21T07:54:20.560449Z"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# 将稀疏矩阵转为一般矩阵查看里面的内容\n",
    "simple_train_dtm.toarray()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.status.busy": "2025-03-21T07:54:20.561394Z",
     "iopub.status.idle": "2025-03-21T07:54:20.561661Z",
     "shell.execute_reply": "2025-03-21T07:54:20.561537Z",
     "shell.execute_reply.started": "2025-03-21T07:54:20.561525Z"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# 结合词汇表和转为得到的矩阵来直观查看内容\n",
    "pd.DataFrame(simple_train_dtm.toarray(), columns=vect.get_feature_names_out())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**2. TfidfVectorizer**  \n",
    "与 CountVectorizer 类似的还有 TfidfVectorizer 。        \n",
    "TF-IDF 算法是创建在这样一个假设之上的：                     \n",
    "对区别文档最有意义的词语应该是那些在文档中出现频率高的词语，因此选择特征空间坐标系取 TF 词频作为测度，就可以体现同类文本的特点。                                    \n",
    "另外考虑到单词区别不同类别的能力，TF-IDF 法认为一个单词出现的文本频数越小，它区别不同类别文本的能力就越大。     \n",
    "因此引入了逆文本频度 IDF 的概念，以 TF 和 IDF 的乘积作为特征空间坐标系的取值测度，并用它完成对权值 TF 的调整，调整权值的目的在于突出重要单词，抑制次要单词。    \n",
    "在本质上 IDF 是一种试图抑制噪声的加权，并且单纯地认为文本频率小的单词就越重要，文本频率大的单词就越无用。    \n",
    " \n",
    "其中 TF、 IDF 和 TF-IDF 的含义如下：\n",
    "+ TF：词频。\n",
    "$$TF(w) = \\frac{词 w 在文档中出现的次数}{文档的总词数}$$\n",
    "+ IDF：逆向文件频率。有些词可能在文本中频繁出现，但并不重要，也即信息量小，如 is, of, that 这些单词，这些单词在语料库中出现的频率也非常大，我们就可以利用这点，降低其权重。\n",
    "$$IDF(w) = ln \\frac{语料库的总文档数}{语料库中词 w 出现的文档数}$$\n",
    "+ TF-ID 综合参数：TF - IDF = TF * IDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.status.busy": "2025-03-21T07:54:20.562904Z",
     "iopub.status.idle": "2025-03-21T07:54:20.563482Z",
     "shell.execute_reply": "2025-03-21T07:54:20.563339Z",
     "shell.execute_reply.started": "2025-03-21T07:54:20.563325Z"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# 导入 TfidfVectorizer\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "tfidf = TfidfVectorizer()\n",
    "# 在训练数据上拟合并将其转为为 tfidf 的稀疏矩阵形式\n",
    "simple_train_dtm = tfidf.fit_transform(simple_train)\n",
    "# 将稀疏矩阵转为一般矩阵\n",
    "simple_train_dtm.toarray()\n",
    "# 结合词汇表和转为得到的矩阵来直观查看内容\n",
    "pd.DataFrame(simple_train_dtm.toarray(), columns=tfidf.get_feature_names_out())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2.3 划分训练集和测试集"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "一般的数据集会划分为两个部分：\n",
    "+ 训练数据：用于训练，构建模型\n",
    "+ 测试数据：在模型检验时使用，用于评估模型是否有效\n",
    "<br>\n",
    "\n",
    "这里划分比例：7:3\n",
    "\n",
    "<br>\n",
    "\n",
    "`sklearn.model_selection.train_test_split(x, y, test_size, random_state )`\n",
    "   +  `x`：数据集的特征值\n",
    "   +  `y`： 数据集的标签值\n",
    "   +  `test_size`： 如果是浮点数，表示测试集样本占比；如果是整数，表示测试集样本的数量。\n",
    "   +  `random_state`： 随机数种子,不同的种子会造成不同的随机采样结果。相同的种子采样结果相同。\n",
    "   +  `return` 训练集的特征值 `x_train` 测试集的特征值 `x_test` 训练集的目标值 `y_train` 测试集的目标值 `y_test`。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.status.busy": "2025-03-21T07:54:20.564282Z",
     "iopub.status.idle": "2025-03-21T07:54:20.564681Z",
     "shell.execute_reply": "2025-03-21T07:54:20.564550Z",
     "shell.execute_reply.started": "2025-03-21T07:54:20.564537Z"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# 构建训练集和测试集\n",
    "from sklearn.model_selection import train_test_split\n",
    "X = np.array(sms.msg_new)\n",
    "y = np.array(sms.label)\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, random_state=42, test_size=0.3)\n",
    "print(\"总共的数据大小\", X.shape)\n",
    "print(\"训练集数据大小\", X_train.shape)\n",
    "print(\"测试集数据大小\", X_test.shape)\n",
    "# 构建私有测试集\n",
    "X_private = np.array(sms_private.msg_new)\n",
    "y_private = np.array(sms_private.label)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**注意：** CountVectorizer 默认会去除长度为 1 的字符串，这会丢失一部分信息，通过将 token_pattern 的属性值改为正则表达式 (?u)\\b\\w+\\b 可以解决这个问题。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.status.busy": "2025-03-21T07:54:20.565495Z",
     "iopub.status.idle": "2025-03-21T07:54:20.566075Z",
     "shell.execute_reply": "2025-03-21T07:54:20.565919Z",
     "shell.execute_reply.started": "2025-03-21T07:54:20.565904Z"
    },
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "# 以 CountVectorizer 为例将数据集向量化\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "# 设置匹配的正则表达式和停用词\n",
    "vect = CountVectorizer(token_pattern=r\"(?u)\\b\\w+\\b\", stop_words=stopwords)\n",
    "X_train_dtm = vect.fit_transform(X_train)\n",
    "X_test_dtm = vect.transform(X_test)\n",
    "X_private_dtm = vect.transform(X_private)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "词袋模型得到的输入特征在两万维以上，为了提高模型训练效果，使用卡方检验选择最具代表性的特征，进行降维。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.status.busy": "2025-03-21T07:54:20.567054Z",
     "iopub.status.idle": "2025-03-21T07:54:20.567435Z",
     "shell.execute_reply": "2025-03-21T07:54:20.567306Z",
     "shell.execute_reply.started": "2025-03-21T07:54:20.567292Z"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# 使用卡方检验选择最具代表性的1000个特征\n",
    "from sklearn.feature_selection import SelectKBest, chi2\n",
    "selector = SelectKBest(chi2, k=1000)\n",
    "X_train_selected = selector.fit_transform(X_train_dtm, y_train)\n",
    "X_test_selected = selector.transform(X_test_dtm)\n",
    "X_private_selected = selector.transform(X_private_dtm)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3.实验题目\n",
    "\n",
    "**题目内容：** 根据一段中文文本（ 200 个中文字符以内），预测这段文本是否为垃圾短信。 "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-04-19T02:53:43.250772Z",
     "iopub.status.busy": "2022-04-19T02:53:43.250107Z",
     "iopub.status.idle": "2022-04-19T02:53:43.253737Z",
     "shell.execute_reply": "2022-04-19T02:53:43.253065Z",
     "shell.execute_reply.started": "2022-04-19T02:53:43.250742Z"
    },
    "tags": []
   },
   "source": [
    "## 3.1 实验要求"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-04-19T02:53:26.023751Z",
     "iopub.status.busy": "2022-04-19T02:53:26.022854Z",
     "iopub.status.idle": "2022-04-19T02:53:26.029182Z",
     "shell.execute_reply": "2022-04-19T02:53:26.028510Z",
     "shell.execute_reply.started": "2022-04-19T02:53:26.023706Z"
    },
    "tags": []
   },
   "source": [
    "实验要求：\n",
    "1. 实验一：使用线性回归和设定阈值直接进行分类，通过梯度下降求解，观察结果。\n",
    "2. 实验二：在实验一的基础上增加正则化项（L1，L2），记录结果，并与实验一的结果进行对比。\n",
    "3. 实验三：使用对数几率回归进行分类，通过梯度下降求解，记录结果，并与实验一、二的结果进行对比。\n",
    "4. 实验四：自行尝试sklearn库中的Logistic Regression或其他模型，调整参数进行训练，记录结果。\n",
    "\n",
    "提交内容：\n",
    "1. 将实验二和实验三Todo部分的代码、实验四的完整代码放到todo.py文件进行提交。\n",
    "2. 将最终在私有测试集上的预测结果'predictions.csv'，下载保存到本地文件predictions.csv，进行提交。\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.2 模型的搭建和训练"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.2.1 实验一"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.status.busy": "2025-03-21T07:54:20.568545Z",
     "iopub.status.idle": "2025-03-21T07:54:20.569063Z",
     "shell.execute_reply": "2025-03-21T07:54:20.568916Z",
     "shell.execute_reply.started": "2025-03-21T07:54:20.568901Z"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# ----------------- 导入相关的库 -----------------\n",
    "from sklearn import metrics\n",
    "\n",
    "# 自定义线性分类器（实际为线性回归结构）\n",
    "class LinearClassifier(paddle.nn.Layer):\n",
    "    def __init__(self, input_dim):\n",
    "        super().__init__()\n",
    "        self.linear = paddle.nn.Linear(input_dim, 1)  # 单输出线性层（无激活函数）\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.linear(x)  # 直接输出线性结果，不使用sigmoid激活函数\n",
    "\n",
    "# 模型训练函数\n",
    "def train_1(X_train_dtm, y_train):\n",
    "    # 参数设置\n",
    "    EPOCHS = 300         # 总训练轮次\n",
    "    BATCH_SIZE = 64      # 批大小\n",
    "    LEARNING_RATE = 0.1  # 学习率\n",
    "    MODEL_PATH = \"linear_classifier.pdparams\"  # 模型保存路径\n",
    "    \n",
    "    # 转换数据格式：稀疏矩阵转密集数组 + 类型转换\n",
    "    X_train = X_train_dtm.toarray().astype('float32')\n",
    "    y_train = y_train.reshape(-1, 1).astype('float32')\n",
    "    \n",
    "    # 初始化模型\n",
    "    model = LinearClassifier(input_dim=X_train.shape[1])\n",
    "    optimizer = paddle.optimizer.SGD(  # 选择随机梯度下降优化器\n",
    "        learning_rate=LEARNING_RATE,\n",
    "        parameters=model.parameters()\n",
    "    )\n",
    "    \n",
    "    best_acc = 0.0\n",
    "    for epoch in range(EPOCHS):\n",
    "        # 训练过程（批训练）\n",
    "        epoch_loss = 0\n",
    "        for i in range(0, len(X_train), BATCH_SIZE):\n",
    "            # 数据分批次加载（避免一次性加载大矩阵）\n",
    "            batch_X = X_train[i:i+BATCH_SIZE]\n",
    "            batch_y = y_train[i:i+BATCH_SIZE]\n",
    "            \n",
    "            # 转换为Paddle Tensor\n",
    "            X_tensor = paddle.to_tensor(batch_X)\n",
    "            y_tensor = paddle.to_tensor(batch_y)\n",
    "\n",
    "            # 前向计算与损失计算（使用MSE）\n",
    "            outputs = model(X_tensor)\n",
    "            loss = paddle.nn.functional.mse_loss(outputs, y_tensor)  # 回归损失\n",
    "            \n",
    "            # 反向传播与参数更新\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            optimizer.clear_grad()  # 清除梯度\n",
    "            epoch_loss += loss.numpy()\n",
    "\n",
    "        # 每个epoch后评估并保存最佳模型\n",
    "        with paddle.no_grad():  # 关闭梯度计算\n",
    "            train_pred = model(paddle.to_tensor(X_train)).numpy()\n",
    "            # 通过阈值0.5将回归输出转为分类结果\n",
    "            current_acc = np.mean((train_pred > 0.5).astype(int) == y_train)\n",
    "            \n",
    "            if current_acc > best_acc:\n",
    "                best_acc = current_acc\n",
    "                paddle.save(model.state_dict(), MODEL_PATH)  # 保存模型参数\n",
    "        \n",
    "        print(f\"Epoch {epoch+1}/{EPOCHS} Loss: {epoch_loss:.4f} | Accuracy: {current_acc:.4f}\")\n",
    "\n",
    "    print(f\"训练完成，模型已保存至：{MODEL_PATH}\")\n",
    "    return MODEL_PATH\n",
    "\n",
    "# 模型评估函数\n",
    "def evaluate_1(model_path, X_test_dtm, y_test):\n",
    "    # 数据预处理（与训练集一致）\n",
    "    X_test = X_test_dtm.toarray().astype('float32')\n",
    "    y_test = y_test.reshape(-1, 1).astype('int')  # 转换为整型标签\n",
    "    \n",
    "    # 初始化模型结构（与训练时一致）\n",
    "    model = LinearClassifier(input_dim=X_test.shape[1])\n",
    "    model.set_state_dict(paddle.load(model_path))  # 加载保存的模型参数\n",
    "    model.eval()  # 设置评估模式（关闭dropout等训练专用层）\n",
    "    \n",
    "    # 进行预测\n",
    "    with paddle.no_grad():\n",
    "        test_outputs = model(paddle.to_tensor(X_test)).numpy()\n",
    "        y_pred = (test_outputs > 0.5).astype(int)\n",
    "    \n",
    "    return y_pred\n",
    "\n",
    "# ----------------- 执行训练与评估 -----------------\n",
    "model_path = train_1(X_train_selected, y_train)\n",
    "y_pred = evaluate_1(model_path, X_test_selected, y_test)\n",
    "# 生成评估报告\n",
    "print(\"\\n在测试集上的混淆矩阵：\")\n",
    "print(metrics.confusion_matrix(y_test, y_pred))\n",
    "\n",
    "print(\"\\n在测试集上的分类结果报告：\")\n",
    "print(metrics.classification_report(y_test, y_pred))\n",
    "\n",
    "print(\"在测试集上的 f1-score：\")\n",
    "print(metrics.f1_score(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.2.2 实验二"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.status.busy": "2025-03-21T07:54:20.570077Z",
     "iopub.status.idle": "2025-03-21T07:54:20.570377Z",
     "shell.execute_reply": "2025-03-21T07:54:20.570228Z",
     "shell.execute_reply.started": "2025-03-21T07:54:20.570216Z"
    },
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "# ----------------- 导入相关的库 -----------------\n",
    "from sklearn import metrics\n",
    "\n",
    "# 模型训练函数\n",
    "def train_2(X_train_dtm, y_train, reg_type='l1', lambda_=0.01):\n",
    "    # 参数设置\n",
    "    EPOCHS = 300\n",
    "    BATCH_SIZE = 64\n",
    "    LEARNING_RATE = 0.1\n",
    "    MODEL_PATH = \"linear_classifier_with_regularization.pdparams\"\n",
    "    \n",
    "    # 转换数据格式\n",
    "    X_train = X_train_dtm.toarray().astype('float32')\n",
    "    y_train = y_train.reshape(-1, 1).astype('float32')\n",
    "    \n",
    "    # 初始化模型（依然使用线性回归结构）\n",
    "    model = LinearClassifier(input_dim=X_train.shape[1])\n",
    "    optimizer = paddle.optimizer.SGD( # 使用随机梯度下降优化器\n",
    "        learning_rate=LEARNING_RATE,\n",
    "        parameters=model.parameters()\n",
    "    )\n",
    "    \n",
    "    best_acc = 0.0\n",
    "    for epoch in range(EPOCHS):\n",
    "        # 训练过程\n",
    "        epoch_loss = 0\n",
    "        for i in range(0, len(X_train), BATCH_SIZE):\n",
    "            batch_X = X_train[i:i+BATCH_SIZE]\n",
    "            batch_y = y_train[i:i+BATCH_SIZE]\n",
    "            \n",
    "            X_tensor = paddle.to_tensor(batch_X)\n",
    "            y_tensor = paddle.to_tensor(batch_y)\n",
    "\n",
    "            outputs = model(X_tensor)\n",
    "            mse_loss = paddle.nn.functional.mse_loss(outputs, y_tensor)\n",
    "            \n",
    "            # 根据reg_type(l1, l2)和lambda计算正则项损失reg_loss（通常不对偏置项正则化）\n",
    "            # --------------------------- TODO ---------------------------------------\n",
    "            # ------------------------------------------------------------------------\n",
    "            \n",
    "            total_loss = mse_loss + reg_loss\n",
    "            \n",
    "            # 反向传播\n",
    "            total_loss.backward()\n",
    "            optimizer.step()\n",
    "            optimizer.clear_grad()\n",
    "            epoch_loss += total_loss.numpy()[0]\n",
    "\n",
    "        # 每个epoch后保存最佳模型\n",
    "        with paddle.no_grad():\n",
    "            train_pred = model(paddle.to_tensor(X_train)).numpy()\n",
    "            current_acc = np.mean((train_pred > 0.5).astype(int) == y_train)\n",
    "            \n",
    "            if current_acc > best_acc:\n",
    "                best_acc = current_acc\n",
    "                paddle.save(model.state_dict(), MODEL_PATH)\n",
    "        \n",
    "        print(f\"Epoch {epoch+1}/{EPOCHS} Loss: {epoch_loss:.4f} | Accuracy: {current_acc:.4f}\")\n",
    "\n",
    "    print(f\"训练完成，模型已保存至：{MODEL_PATH}\")\n",
    "    return MODEL_PATH\n",
    "\n",
    "# 模型评估函数\n",
    "def evaluate_2(model_path, X_test_dtm, y_test):\n",
    "    X_test = X_test_dtm.toarray().astype('float32')\n",
    "    y_test = y_test.reshape(-1, 1).astype('int')\n",
    "    \n",
    "    # 初始化模型结构\n",
    "    model = LinearClassifier(input_dim=X_test.shape[1])\n",
    "    model.set_state_dict(paddle.load(model_path))\n",
    "    model.eval()\n",
    "    \n",
    "    # 进行预测\n",
    "    with paddle.no_grad():\n",
    "        test_outputs = model(paddle.to_tensor(X_test)).numpy()\n",
    "        y_pred = (test_outputs > 0.5).astype(int)\n",
    "    \n",
    "    return y_pred\n",
    "\n",
    "# 训练模型（正则化项和权重自行尝试）\n",
    "model_path = train_2(X_train_selected, y_train, reg_type='l2', lambda_=0.01)\n",
    "\n",
    "# 评估模型\n",
    "y_pred = evaluate_2(model_path, X_test_selected, y_test)\n",
    "\n",
    "# 生成评估报告\n",
    "print(\"\\n在测试集上的混淆矩阵：\")\n",
    "print(metrics.confusion_matrix(y_test, y_pred))\n",
    "\n",
    "print(\"\\n在测试集上的分类结果报告：\")\n",
    "print(metrics.classification_report(y_test, y_pred))\n",
    "\n",
    "print(\"在测试集上的 f1-score：\")\n",
    "print(metrics.f1_score(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.2.3 实验三"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.status.busy": "2025-03-21T07:54:20.571493Z",
     "iopub.status.idle": "2025-03-21T07:54:20.571790Z",
     "shell.execute_reply": "2025-03-21T07:54:20.571645Z",
     "shell.execute_reply.started": "2025-03-21T07:54:20.571633Z"
    },
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "# ----------------- 导入相关的库 -----------------\n",
    "from sklearn import metrics\n",
    "\n",
    "# 逻辑回归分类器\n",
    "class LogisticClassifier(paddle.nn.Layer):\n",
    "    def __init__(self, input_dim):\n",
    "        super().__init__()\n",
    "        self.linear = paddle.nn.Linear(input_dim, 1)  # 线性变换层\n",
    "        self.sigmoid = paddle.nn.Sigmoid()  # 将线性输出转为概率的激活函数\n",
    "\n",
    "    def forward(self, x):\n",
    "        # 将线性结果通过sigmoid得到概率值\n",
    "        # --------------------------- TODO ---------------------------------------\n",
    "        # ------------------------------------------------------------------------\n",
    "\n",
    "# 模型训练函数\n",
    "def train_3(X_train_dtm, y_train):\n",
    "    # 参数设置\n",
    "    EPOCHS = 300\n",
    "    BATCH_SIZE = 64\n",
    "    LEARNING_RATE = 0.1 \n",
    "    MODEL_PATH = \"logistic_classifier.pdparams\"\n",
    "\n",
    "    # 数据预处理\n",
    "    X_train = X_train_dtm.toarray().astype('float32')\n",
    "    y_train = y_train.reshape(-1, 1).astype('float32')\n",
    "\n",
    "    # 模型初始化\n",
    "    model = LogisticClassifier(input_dim=X_train.shape[1])\n",
    "    optimizer = paddle.optimizer.SGD(  # 使用随机梯度下降优化器\n",
    "        learning_rate=LEARNING_RATE,\n",
    "        parameters=model.parameters()\n",
    "    )\n",
    "    \n",
    "    best_acc = 0.0\n",
    "    for epoch in range(EPOCHS):\n",
    "        epoch_loss = 0 \n",
    "        for i in range(0, len(X_train), BATCH_SIZE):\n",
    "            # 数据批次划分\n",
    "            batch_X = X_train[i:i+BATCH_SIZE]\n",
    "            batch_y = y_train[i:i+BATCH_SIZE]\n",
    "            \n",
    "            X_tensor = paddle.to_tensor(batch_X)\n",
    "            y_tensor = paddle.to_tensor(batch_y)\n",
    "\n",
    "            # 使用二元交叉熵损失\n",
    "            # --------------------------- TODO ---------------------------------------\n",
    "            # ------------------------------------------------------------------------\n",
    "            \n",
    "            # 反向传播与参数更新\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            optimizer.clear_grad()\n",
    "            epoch_loss += loss.numpy().item()\n",
    "\n",
    "        # 模型验证与保存\n",
    "        with paddle.no_grad():\n",
    "            y_pred = model(paddle.to_tensor(X_train)).numpy()\n",
    "            current_acc = np.mean((y_pred > 0.5).astype(int) == y_train)\n",
    "            \n",
    "            if current_acc > best_acc:\n",
    "                best_acc = current_acc\n",
    "                paddle.save(model.state_dict(), MODEL_PATH)\n",
    "        \n",
    "        print(f\"Epoch {epoch+1}/{EPOCHS} Loss: {epoch_loss:.4f} | Accuracy: {current_acc:.4f}\")\n",
    "\n",
    "    print(f\"训练完成，模型已保存至：{MODEL_PATH}\")\n",
    "    return MODEL_PATH\n",
    "\n",
    "# 模型评估函数\n",
    "def evaluate_3(model_path, X_test_dtm, y_test):\n",
    "    # 数据预处理\n",
    "    X_test = X_test_dtm.toarray().astype('float32')\n",
    "    y_test = y_test.reshape(-1, 1).astype('int')\n",
    "\n",
    "    # 模型加载\n",
    "    model = LogisticClassifier(input_dim=X_test.shape[1])\n",
    "    model.set_state_dict(paddle.load(model_path))\n",
    "    model.eval()\n",
    "\n",
    "    # 预测执行\n",
    "    with paddle.no_grad():\n",
    "        y_pred = (model(paddle.to_tensor(X_test)).numpy() > 0.5).astype(int)\n",
    "    \n",
    "    return y_pred\n",
    "\n",
    "# ----------------- 执行训练与评估 -----------------\n",
    "model_path = train_3(X_train_selected, y_train)  # 启动训练\n",
    "y_pred = evaluate_3(model_path, X_test_selected, y_test)  # 执行评估\n",
    "# 评估指标输出\n",
    "print(\"\\n在测试集上的混淆矩阵：\")\n",
    "print(metrics.confusion_matrix(y_test, y_pred))\n",
    "\n",
    "print(\"\\n在测试集上的分类结果报告：\")\n",
    "print(metrics.classification_report(y_test, y_pred))\n",
    "\n",
    "print(\"在测试集上的 f1-score：\")\n",
    "print(metrics.f1_score(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.2.4 实验四"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.status.busy": "2025-03-21T07:54:20.572597Z",
     "iopub.status.idle": "2025-03-21T07:54:20.573195Z",
     "shell.execute_reply": "2025-03-21T07:54:20.573040Z",
     "shell.execute_reply.started": "2025-03-21T07:54:20.573025Z"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn import metrics\n",
    "import joblib\n",
    "\n",
    "def train_4(X_train_dtm, y_train):\n",
    "    # 自行探索sklearn库里的Logistic Regression或别的模型，观察效果如何\n",
    "    model = LogisticRegression(max_iter=300, solver='lbfgs')  \n",
    "    model.fit(X_train_dtm, y_train)  # 训练模型\n",
    "    model_path = \"sklearn.pkl\"\n",
    "    joblib.dump(model, model_path)  # 保存模型\n",
    "    print(f\"训练完成，模型已保存至：{model_path}\")\n",
    "    \n",
    "    return model_path\n",
    "\n",
    "def evaluate_4(model_path, X_test_dtm, y_test):\n",
    "    model = joblib.load(model_path)  # 加载模型\n",
    "    y_pred = model.predict(X_test_dtm)  # 预测\n",
    "\n",
    "    return y_pred\n",
    "\n",
    "# 训练和评估\n",
    "model_path = train_4(X_train_selected, y_train)\n",
    "y_pred = evaluate_4(model_path, X_test_selected, y_test)\n",
    "# 评估指标输出\n",
    "print(\"\\n在测试集上的混淆矩阵：\")\n",
    "print(metrics.confusion_matrix(y_test, y_pred))\n",
    "\n",
    "print(\"\\n在测试集上的分类结果报告：\")\n",
    "print(metrics.classification_report(y_test, y_pred))\n",
    "\n",
    "print(\"在测试集上的 f1-score：\")\n",
    "print(metrics.f1_score(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.3 模型的预测"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. 请加载你认为训练最佳的模型，即请按要求选择selected_model。\n",
    "2. 将最终在测试数据集上的预测结果'predictions.csv'，下载到本地，进行提交。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.status.busy": "2025-03-21T07:54:20.574072Z",
     "iopub.status.idle": "2025-03-21T07:54:20.574492Z",
     "shell.execute_reply": "2025-03-21T07:54:20.574358Z",
     "shell.execute_reply.started": "2025-03-21T07:54:20.574344Z"
    },
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "# 将真实标签与预测结果导出为CSV文件\n",
    "def export_predictions(y_true, y_pred, filename='predictions.csv'):\n",
    "    df = pd.DataFrame({\n",
    "        'True_Label': y_true.flatten(),\n",
    "        'Predicted_Label': y_pred.flatten()\n",
    "    })\n",
    "    df.to_csv(filename, index=False)\n",
    "    print(f\"预测结果已保存至：{filename}\")\n",
    "\n",
    "# 根据模型类型选择并执行预测\n",
    "def select_and_predict(model_type, X_test, y_test):\n",
    "    \"\"\"\n",
    "    参数：\n",
    "        model_type - 模型类型字符串 ('linear', 'regularized', 'logistic', 'sklearn')\n",
    "        X_test - 测试集特征矩阵\n",
    "        y_test - 测试集标签\n",
    "    返回：\n",
    "        预测结果数组\n",
    "    \"\"\"\n",
    "    # 模型路径映射\n",
    "    model_paths = {\n",
    "        'linear': 'linear_classifier.pdparams',\n",
    "        'regularized': 'linear_classifier_with_regularization.pdparams',\n",
    "        'logistic': 'logistic_classifier.pdparams',\n",
    "        'sklearn': 'sklearn.pkl'\n",
    "    }\n",
    "    \n",
    "    # 根据类型选择评估函数\n",
    "    if model_type == 'logistic':\n",
    "        y_pred = evaluate_3(model_paths[model_type], X_test, y_test)\n",
    "    elif model_type == 'sklearn':\n",
    "        y_pred = evaluate_4(model_paths[model_type], X_test, y_test)\n",
    "    else:\n",
    "        y_pred = evaluate_2(model_paths[model_type], X_test, y_test) if model_type == 'regularized' else evaluate_1(model_paths[model_type], X_test, y_test)\n",
    "    \n",
    "    return y_pred\n",
    "\n",
    "    \n",
    "selected_model = 'logistic'  # 在linear, regularized, logistic和sklearn中选择\n",
    "predictions = select_and_predict(selected_model, X_private_selected, y_private)\n",
    "\n",
    "# 导出结果\n",
    "export_predictions(y_private, predictions, 'predictions.csv')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  },
  "toc-autonumbering": false
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
