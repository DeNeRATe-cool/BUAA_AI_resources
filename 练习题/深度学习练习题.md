明白了。我将根据你提供的深度学习考试考纲，设计一份共约30道题的测试题集，涵盖单选题、多选题、填空题、简答题和推导/计算题五种类型，难度较高，并覆盖以下主题：

- 深度学习与机器学习的区别与联系
- 激活函数、前馈神经网络、卷积神经网络、自注意力机制
- 卷积网络和注意力机制的基本原理和计算方式
- 模型训练策略（如过拟合与欠拟合判断与处理）
- Transformer、BERT、GPT 等模型的核心思想
	 我会为每道题提供详细参考答案与解析，稍后将测试题整理好后呈现给你。

# 深度学习期末考试题

## 单选题（每题只有一个正确答案，共 10 题）

1. **深度学习与传统机器学习的一个显著区别是：**
	 A. 深度学习通常需要更多的特征工程；
	 B. 深度学习通常需要更大的数据集以充分训练；
	 C. 机器学习模型一般包含更深的网络结构；
	 D. 深度学习模型训练速度通常更快。
	 **答案：B。** 解析：深度学习模型参数众多，容易过拟合，因此通常需要海量数据才能很好地训练；而传统机器学习往往数据需求相对较少【来源】。选项 A 说法错误，深度学习减少了手工特征工程；C 错误，深度学习模型结构往往更深；D 错误，深度模型训练一般更慢。
2. **以下哪种激活函数最不容易出现梯度消失问题？**
	 A. Sigmoid；
	 B. Tanh；
	 C. ReLU（修正线性单元）；
	 D. Softmax。
	 **答案：C。** 解析：ReLU 在正区间具有常数梯度（为 1），不容易饱和，从而缓解梯度消失问题；而 Sigmoid 和 Tanh 在数值饱和区梯度接近零易导致梯度消失。Softmax 常用作输出层激活，不用于隐藏层【来源】。
3. **如果一个多层全连接神经网络在各层之间不使用任何非线性激活函数，则该网络整体等价于：**
	 A. 一个线性模型；
	 B. 一个非线性模型；
	 C. 权重矩阵之和；
	 D. 恒为零的输出。
	 **答案：A。** 解析：不使用非线性激活时，各层的线性变换可合并成一个线性变换（两个线性矩阵相乘仍为线性变换），相当于一个单层的线性模型【来源】。
4. **下面关于卷积神经网络 (CNN) 的说法错误的是：**
	 A. 卷积层的滤波器参数在空间上共享；
	 B. 随着卷积网络层数增加，可以捕捉更高层次的抽象特征；
	 C. 每个卷积核会产生一个输出特征图；
	 D. 相比其他模型，CNN 对训练数据量的要求通常不高。
	 **答案：D。** 解析：CNN 由于参数量较大，也需要较多的数据来充分训练，选项 D 说法错误。A、B、C 正确：卷积使用参数共享降低复杂度；多层网络能提取层次化特征；每个卷积核通过滑动生成一个输出特征图【来源】。
5. **下列关于自注意力(Self-Attention)机制的描述中，错误的是：**
	 A. 自注意力通过线性变换生成 Query、Key、Value 矩阵；
	 B. 注意力权重通过计算 Q 和 K 的点积并经过 Softmax 得到；
	 C. 最终输出通过对 Value 进行加权求和得到；
	 D. 自注意力无需位置编码即可捕捉序列元素的位置信息。
	 **答案：D。** 解析：自注意力机制本身不包含位置信息，需要通过位置编码(Position Encoding)来引入序列顺序信息，选项 D 错误。A、B、C 正确描述了自注意力的计算过程【来源】。
6. **当模型在训练集和测试集上都表现不佳时（损失较高、准确率较低），通常表明：**
	 A. 过拟合；
	 B. 欠拟合（拟合不足）；
	 C. 参数初始化错误；
	 D. 输入数据泄露。
	 **答案：B。** 解析：若训练集表现就很差，说明模型容量不足或训练不到位，即欠拟合；过拟合情况是训练集精度高而测试集精度低；选项 C、D 与题意无关。
7. **下面哪一个方法对于缓解神经网络过拟合没有帮助？**
	 A. 降低网络复杂度（减少参数）；
	 B. 增加训练数据量；
	 C. 使用更大的学习率；
	 D. 添加正则化（如 L2 正则项）；
	 E. 使用 Dropout。
	 **答案：C。** 解析：增大学习率并不能防止过拟合，反而可能导致训练不稳定。选项 A、B、D、E 都是常见的防止过拟合的方法：简化模型、增加数据、正则化和 Dropout 都能有效缓解过拟合。
8. **Transformer 模型的一个关键特点是：**
	 A. 使用循环神经网络来处理序列数据；
	 B. 完全依赖注意力机制，不使用 RNN/CNN；
	 C. 不使用任何位置编码；
	 D. 在每层只使用单头注意力机制。
	 **答案：B。** 解析：Transformer 的核心在于使用多头自注意力机制处理序列，无需卷积或循环结构。选项 A、C、D 均不正确（它不使用 RNN/CNN，实际上使用位置编码，且采用多头注意力）。
9. **关于 BERT 模型的训练目标，以下说法正确的是：**
	 A. 仅使用自回归语言模型目标；
	 B. 使用掩码语言模型(Masked LM)和下一句预测(NSP)任务；
	 C. 仅使用下一句预测任务；
	 D. 主要用于解码器阶段的文本生成。
	 **答案：B。** 解析：BERT 在预训练时同时采用掩码语言模型（随机掩盖输入中的部分词语）和下一句预测任务来训练双向编码器【来源】。选项 A、C、D 与 BERT 的实际训练目标不符。
10. **GPT 系列模型的特点是：**
	 A. 使用双向编码器结构；
	 B. 基于自回归（左到右）语言模型进行训练；
	 C. 预训练阶段使用掩码语言模型；
	 D. 模型在处理序列时依赖于双向注意力。
	 **答案：B。** 解析：GPT 属于自回归语言模型，采用单向（从左到右）生成。选项 A、C、D 描述均不符合 GPT（A, C 适用于 BERT；GPT 使用单向注意力）。

## 多选题（每题可有多个正确答案，共 6 题）

1. **下列哪些属于常见的非线性激活函数？**
	 A. Sigmoid；
	 B. Softmax；
	 C. ReLU；
	 D. 线性函数；
	 E. Tanh。
	 **答案：A、C、E。** 解析：Sigmoid、ReLU 和 Tanh 都是常用的非线性激活函数；Softmax 通常作为输出层做归一化处理；线性函数不是非线性激活函数。
2. **以下哪些方法有助于防止神经网络过拟合？**
	 A. 降低网络复杂度（减少参数）；
	 B. 增加训练数据量；
	 C. 使用更高的学习率；
	 D. 添加正则化（如 L2 正则项）；
	 E. 使用 Dropout。
	 **答案：A、B、D、E。** 解析：降低模型复杂度、增大训练数据、添加正则化以及应用 Dropout 都是防止过拟合的有效策略；选项 C 不具有防止过拟合的作用。
3. **关于卷积神经网络 (CNN)，下列说法正确的是：**
	 A. 每个卷积核对应一个输出特征图；
	 B. 卷积操作利用局部感受野并进行参数共享；
	 C. 卷积网络无需固定输入尺寸；
	 D. 随着网络深度的增加可以提取更高级的特征；
	 E. 卷积层通常不需要使用激活函数。
	 **答案：A、B、D。** 解析：A 正确，每个卷积核（滤波器）会生成一个输出特征图；B 正确，卷积使用局部感受野和参数共享以降低参数量；D 正确，多层网络可以抽取更抽象的特征；C 错误，虽然卷积层本身对输入尺寸灵活，但完整 CNN（含全连接层）往往需要固定尺寸；E 错误，卷积后通常也会加激活函数以引入非线性。
4. **关于 Transformer 中的多头自注意力机制，下列说法正确的是：**
	 A. 多头注意力允许模型在不同子空间并行关注不同位置关系；
	 B. 在自注意力中，Query、Key、Value 三者来自同一输入序列；
	 C. 最终输出是将各个注意力头的输出相加得到；
	 D. 多头注意力将不同头的结果拼接（concatenate）并线性映射得到最终输出。
	 **答案：A、B、D。** 解析：A 正确，多头机制使模型能够从不同表示子空间关注不同模式；B 正确，在自注意力(Self-Attention)中 Q、K、V 都是由同一输入序列通过不同线性变换得到；D 正确，多头注意力会将各头结果拼接后通过线性层得到最终输出；C 错误，各头输出是拼接而不是简单相加。
5. **关于 BERT 和 GPT 模型，下列说法正确的是：**
	 A. BERT 使用双向（bidirectional）编码器；
	 B. GPT 使用自回归（单向）解码器；
	 C. BERT 在预训练中使用 Masked LM；
	 D. GPT 可用于无监督特征提取；
	 E. BERT 更适合生成式任务。
	 **答案：A、B、C、D。** 解析：A 正确，BERT 是双向 Transformer 编码器；B 正确，GPT 是基于自回归（左到右）解码器；C 正确，BERT 预训练使用掩码语言模型；D 正确，GPT 作为语言模型也可视为无监督特征提取器；E 错误，BERT 通常用于理解任务，不擅长直接生成文本。
6. **关于欠拟合和过拟合，以下说法正确的是：**
	 A. 欠拟合模型在训练集和验证集上的误差都很高；
	 B. 过拟合模型在训练集上的误差低而在验证集上误差高；
	 C. 增加模型容量（增加参数）通常可缓解欠拟合；
	 D. 增加模型容量可以缓解过拟合；
	 E. 数据量过大会导致过拟合。
	 **答案：A、B、C。** 解析：A 正确，欠拟合时模型泛化能力差，两者误差都高；B 正确，过拟合时模型在训练集上表现很好但在验证集上表现差；C 正确，增加容量可以让模型拟合更复杂的规律，有助于减少欠拟合；D 错误，增加容量通常会加剧过拟合；E 错误，数据量过大反而有助于减轻过拟合。

## 填空题（4 题）

1. 常见的非线性激活函数有 ***\**\*\*\*、\*\*\*\*\**** 和 ________。
	 **答案：** ReLU、Sigmoid、Tanh。
	 **解析：** ReLU（修正线性单元）、Sigmoid（S 形函数）和 Tanh（双曲正切）是最常用的非线性激活函数，它们为神经网络引入了非线性特征映射【来源】。
2. 在卷积运算中，如果输入尺寸为 64×64，卷积核尺寸为 5×5，步长=1，填充(padding)=0，则输出尺寸为 ***\**\*\*\*×\*\*\*\*\****。
	 **答案：** 60×60。
	 **解析：** 输出宽度或高度 = (输入大小 – 卷积核大小 + 2×Padding) / 步长 + 1 = (64–5+0)/1+1 = 60，所以输出是 60×60【来源】。
3. Transformer 模型中的多头自注意力机制会对输入序列做线性变换生成三个矩阵，这三个矩阵分别代表 ***\**\*\*\*、\*\*\*\*\**** 和 ________ 向量。
	 **答案：** Query、Key、Value。
	 **解析：** 在自注意力模块中，输入向量通过三个独立的线性变换生成 Query、Key、Value，这三者用于计算注意力权重并组合输出【来源】。
4. BERT 模型预训练时使用的两种任务是：掩码语言模型和 ________。
	 **答案：** 下一句预测（Next Sentence Prediction）。
	 **解析：** BERT 的预训练目标包括 MLM（Masked Language Model）和 NSP（Next Sentence Prediction）任务，后者用于让模型学习句子间关系【来源】。

## 简答题（5 题）

1. **简述深度学习与传统机器学习的区别与联系。**
	 **答案：** 深度学习是机器学习的一个分支，二者都是数据驱动的学习方法。从模型结构上看，深度学习采用多层非线性神经网络，层数远多于传统机器学习模型（如决策树、SVM等）；深度学习擅长自动学习特征表示，减少了人工特征工程的需求，而传统机器学习往往需要依赖手工设计特征。【来源】联系方面，深度学习模型（神经网络）仍然使用优化算法（如梯度下降）进行训练，本质上仍是机器学习。总体上，深度学习可以看作是使用深度神经网络的大规模机器学习方法，能够更好地挖掘数据中的复杂模式，但对数据量和计算资源的需求更高。

2. **简述卷积神经网络的前向计算过程和特点。**
	 **答案：** 卷积神经网络的前向计算包括：输入层接收原始数据（例如图像），随后通过一系列卷积层、激活层和池化层逐步提取特征。在卷积层中，卷积核（滤波器）在输入图像上滑动，对覆盖区域做线性加权求和，提取局部空间特征；激活函数（如ReLU）对卷积结果做非线性变换；池化层（如最大池化）对特征图进行下采样，保留显著特征并减少尺寸。多个卷积块堆叠能够提取从低级到高级的层次化特征。卷积层参数共享和局部感受野的特点使得 CNN 参数更少且具有平移不变性【来源】。最后，一般通过全连接层将提取的特征映射到输出类别。

3. **简述自注意力机制的计算过程及其优势。**
	 **答案：** 自注意力机制通过计算序列中每个元素与其他元素的相关性来进行信息聚合：对输入序列中的每个位置，模型首先通过线性变换生成对应的 Query、Key、Value 向量，然后计算 Query 与所有 Key 的点积，相似度经过缩放（除以 $\sqrt{d_k}$）后通过 Softmax 转换为权重。接着用这些注意力权重对所有 Value 向量加权求和，得到该位置的输出表示。与传统RNN不同，自注意力能够并行计算、捕获长距离依赖，并且对不同位置给予不同关注度，从而更灵活地建模序列全局信息【来源】。

4. **如何判断模型过拟合或欠拟合？常用的防止过拟合方法有哪些？**
	 **答案：** 通过观察训练集和验证集（测试集）上的性能差异来判断：如果训练误差很低而验证误差很高，则说明模型**过拟合**（模型在训练集上学习得很好但对新数据泛化差）；如果训练误差较高且验证误差也高，则可能是**欠拟合**（模型能力不足，无法学习到数据规律）。防止过拟合的常用方法包括：① 增加训练数据（更多样本帮助模型泛化）；② 添加正则化（如 L1/L2 正则化、Dropout）限制模型复杂度；③ 使用简单模型（减少参数）或早停（Early Stopping）策略；④ 数据扩增（Data Augmentation）人为生成更多样本【来源】。

5. **简述 Transformer 的基本架构以及 BERT、GPT 之间的区别。**
	 **答案：** Transformer 架构由编码器 (Encoder) 和解码器 (Decoder) 组成，每个编码器/解码器层包括多头自注意力机制和前馈神经网络两部分。编码器接收输入序列并通过注意力捕获上下文表示，解码器在生成时会同时关注之前生成的输出和编码器输出。与传统的 RNN 不同，Transformer 完全基于注意力机制，并行性更高。

	BERT 和 GPT 都基于 Transformer，但结构不同：BERT 是双向 Transformer 编码器，只使用编码器部分；GPT 是基于单向（从左到右）Transformer 解码器。训练目标也不同：BERT 在预训练时使用掩码语言模型和下一句预测，使其更适合理解任务；GPT 则使用自回归语言模型目标，按顺序生成下一个词，更适合文本生成。换言之，BERT 常用于提取句子或文本的语义特征，GPT 常用于生成文本或续写任务【来源】。

## 推导/计算题（5 题）

1. **已知输入图像大小为 28×28，通道数 1；使用大小为 5×5 的卷积核，步长=1，无填充(padding=0)，卷积层输出通道数为 6。请计算此卷积层的输出尺寸和参数数量。**
	 **解答：**

- 输出尺寸：按公式 $(W - K + 2P)/S + 1$ 计算，宽高均为 $(28 - 5 + 0)/1 + 1 = 24$，所以输出的尺寸为 $24\times24$。
- 参数数量：每个卷积核大小为 $5\times5$，输入通道数为 1，输出通道数为 6，所以权重参数为 $5\times5\times1\times6 = 150$ 个。此外每个输出通道有一个偏置，共 6 个。因此总参数数目为 $150 + 6 = 156$。【解析：卷积层参数计算包括权重和偏置，卷积参数共享使得每个卷积核只需 5×5×（输入通道）个权重】

1. **给定一个长度为 2 的输入序列，其 Query 向量为 $Q=[1,0]$，Key 向量集合为 $K_1=[1,0]$，$K_2=[0,1]$，对应的 Value 向量为 $V_1=[1,2]$，$V_2=[3,4]$。假设无缩放（$\sqrt{d_k}$），计算该位置的自注意力输出。**
	 **解答：**

- 计算注意力权重：$\text{score}_1 = Q\cdot K_1 = (1,0)\cdot(1,0) = 1$，$\text{score}_2 = Q\cdot K_2 = (1,0)\cdot(0,1) = 0$.
- 归一化权重：对 score 做 Softmax：$w_1 = \frac{e^{1}}{e^{1}+e^{0}} = \frac{e}{e+1} \approx 0.73$，$w_2 = \frac{e^{0}}{e^{1}+e^{0}} = \frac{1}{e+1} \approx 0.27$.
- 计算输出：$O = w_1 V_1 + w_2 V_2 = 0.73\times[1,2] + 0.27\times[3,4] = [0.73+0.81,;1.46+1.08] = [1.54,;2.54]$（近似值）。
	 **解析：** 这是将 Query 与 Keys 点积得到相似度，通过 Softmax 转换为权重后对 Values 加权求和得到输出。

1. **证明：不含非线性激活函数的多层全连接网络等价于单层线性模型。**
	 **解答：** 设两层全连接网络，无激活函数，第一层权重 $W_1$、偏置 $b_1$，第二层权重 $W_2$、偏置 $b_2$。输入为 $x$，则输出为：$y = W_2(W_1 x + b_1) + b_2 = (W_2 W_1)x + (W_2 b_1 + b_2)$。
	 定义 $W = W_2 W_1$，$b = W_2 b_1 + b_2$，则 $y = W x + b$，这与单层线性模型形式相同。因此无论网络层数多少，只要没有非线性激活，最终都相当于一个线性变换【解析：多层线性变换可以合并成一个等效的线性变换，所以网络无法表达非线性函数】。
2. **对于 softmax 输出层和交叉熵损失函数 $L = -\sum_{j}t_j \log p_j$，推导损失对输入 $z_k$ 的梯度表达式，其中 $p_j = \frac{e^{z_j}}{\sum_i e^{z_i}}$。**
	 **解答：**
	 交叉熵关于 $z_k$ 的梯度：

$$
\frac{\partial L}{\partial z_k}   = -\sum_j t_j \frac{\partial}{\partial z_k}(\log p_j)  = -\sum_j t_j \frac{1}{p_j} \frac{\partial p_j}{\partial z_k}.
$$

又因为 $p_j$ 对 $z_k$ 的导数为：

$\frac{\partial p_j}{\partial z_k} = p_j(\delta_{jk} - p_k),$

其中 $\delta_{jk}$ 为 Kronecker delta (当 $j=k$ 时为1，否则为0)。代入得：
$$
\frac{\partial L}{\partial z_k}   = -\sum_j t_j (\delta_{jk} - p_k)   = -t_k + \sum_j t_j p_k   = p_k - t_k
$$
因为 $\sum_j t_j =1$（独热向量标签）。**解析：** 交叉熵与 softmax 的复合导数结果为预测概率减去真实标签（$p_k - t_k$），这是深度学习中常用的结论【来源】。

1. **在 Transformer 的多头注意力中，若模型隐藏维度 $d_\text{model}=512$，头数 $h=8$，求每个头的维度 $d_k$。**
	 **解答：** Transformer 中通常有关系 $d_\text{model} = h \times d_k$。已知 $d_\text{model}=512$，$h=8$，因此 $d_k = 512/8 = 64$。**解析：** 这表示每个注意力头在查询、键、值上的线性投影维度为 64；所有头的输出再拼接回 512 维输出【来源】。