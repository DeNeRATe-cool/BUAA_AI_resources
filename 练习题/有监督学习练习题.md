明白了。我将为本科生设计一套有监督学习相关的期末考试试卷，共约20道题，涵盖选择题、填空题、简答题和计算题，并提供每道题的标准答案和格式规范清晰的呈现。
 我完成后会将试卷和答案展示给您。

# 有监督学习期末考试试卷

## 一、选择题

1. 监督学习的学习过程依赖于 ____ 数据。B
	 A. 无标签
	 B. 带标签
	 C. 验证集
	 D. 测试集
2. 下列损失函数中，适用于分类任务的有（多选）：ABD
	 A. 0-1损失
	 B. 交叉熵损失
	 C. 均方误差
	 D. 铰链损失（hinge loss）
3. 下列关于正则化的说法中，正确的是（多选）：ABD
	 A. 正则化可以缓解过拟合
	 B. L1正则化通常可产生稀疏解
	 C. 正则化增加模型的自由度
	 D. 正则化可以减少模型的方差
4. 关于梯度下降方法，下列描述正确的是（多选）：AC
	 A. 需要选择学习率超参数
	 B. 仅适用于凸函数优化
	 C. 如果目标函数是凸的，可找到全局最优
	 D. 需要计算损失函数的二阶导数
5. 决策树选择划分特征时，使用信息增益(Information Gain)，其本质是衡量划分前后样本集合 ____ 的变化量。A
	 A. 信息熵（Entropy）
	 B. 基尼系数（Gini系数）
	 C. 样本均方误差
	 D. 预测误差率
6. 支持向量机（SVM）的目标是通过寻找分类超平面，最大化 ____。B
	 A. 分类错误率
	 B. 分类间隔（Margin）
	 C. 支持向量数量
	 D. 训练准确率

## 二、填空题

1. 监督学习的训练数据由输入特征和 __输出标签__ 组成。
2. 线性回归常用的损失函数是 __均方误差（MSE）__ 。
3. 为了防止过拟合，通常在损失函数中加入模型参数的 __L1范数或L2范数__ 项作为正则化项。
4. 决策树划分时，信息增益衡量的是划分前后样本集合 __不确定性（信息熵）__ 的减少量。

## 三、简答题

1. **什么是监督学习？标注数据在监督学习中起什么作用？**
	 【答案】监督学习是一种使用带有输入-输出标签的训练数据来学习模型的机器学习范式。标注数据包含每个样本的输入特征和对应的输出标签，模型通过对这些有标签的数据进行学习，得到输入到输出的映射关系。训练过程中，模型通过最小化预测值与真实标签之间的损失（如均方误差或交叉熵）来调整参数，从而使模型能够对新样本进行准确预测。
2. **什么是损失函数？它在模型训练中起什么作用？**
	 【答案】损失函数用于衡量模型预测值与实际标签之间的误差或差异。在训练过程中，损失函数（如均方误差、交叉熵等）对每个样本计算损失值，并通过求所有样本的平均或总和来得到总体损失。优化算法（如梯度下降）通过最小化损失函数来调整模型参数，使模型的预测结果尽量接近真实标签，从而提高模型性能。
3. **简要描述梯度下降算法的基本思想。**
	 【答案】梯度下降是一种迭代优化算法，用于最小化损失函数。基本思路是从参数的初始值开始，不断按照损失函数在当前参数处的负梯度方向更新参数。每次迭代根据参数的梯度以及学习率的大小调整步长，使损失函数逐渐减小。通过多次迭代，梯度下降可以使参数收敛到使损失最小的位置（对于凸问题时为全局最优点）。
4. **什么是正则化？常见的正则化方法有哪些，它们有什么作用？**
	 【答案】正则化是在损失函数中加入额外的惩罚项，以控制模型的复杂度并防止过拟合。常见的正则化方法包括L1正则化（Lasso）和L2正则化（Ridge）。L1正则化通过加入参数的绝对值之和作为惩罚项，可以促使一些参数变为零，从而得到稀疏解；L2正则化通过加入参数的平方和作为惩罚项，可以让参数变小但不完全为零，有助于减小模型的方差。正则化的作用是增加模型的偏差以换取方差的减少，从而提高模型的泛化能力。
5. **决策树如何选择最优划分特征？请简要说明信息增益和基尼系数的作用。**
	 【答案】决策树在构建过程中需要选择一个最佳特征来划分数据集，以使得每个子节点的数据更加“纯”。信息增益基于信息熵衡量划分前后样本集的纯度变化：选择划分后信息熵下降最多的特征作为划分依据。基尼系数则是另一种不纯度度量，值越小表示数据越纯；基尼系数计算公式中考虑了各类别的概率，通过计算划分前后的基尼不纯度变化来选择划分特征。通常，信息增益和基尼系数都用于评估每个特征的划分效果，信息增益更关注熵的减少量，而基尼系数关注类别纯度。
6. **简要说明支持向量机（SVM）的基本思想。**
	 【答案】支持向量机（SVM）是一种二分类模型，其基本思想是寻找一个能够将两类数据分开的最佳超平面。SVM通过最大化超平面与两类最近数据点（支持向量）之间的间隔来提高分类的鲁棒性。对于线性可分问题，SVM求解使间隔最大的直线或超平面；对于非线性可分问题，SVM可以借助核函数将数据映射到高维空间，在高维空间中寻找线性可分的超平面。通过最大化分类间隔，SVM具有较好的泛化性能。
7. **简要说明集成学习中Bagging和Boosting的基本思想及区别。**
	 【答案】集成学习是通过组合多个学习器来提高整体性能的方法。Bagging（自助采样法）通过对数据进行多次有放回抽样，生成多个不同的子训练集，并在每个子集上训练一个弱学习器，最终将这些学习器的预测结果进行平均或投票，从而降低模型的方差并提高稳定性。Boosting（提升方法）则是按顺序训练弱学习器，每轮训练时对前一轮被错误分类的样本增加权重，使后续学习器更加关注难分类的样本，最终将多个弱学习器按照其效果加权组合，从而降低整体偏差。简而言之，Bagging侧重并行构建多个多样性的模型来减少过拟合，而Boosting侧重串行地改进模型并纠正错误样本来提升准确度。

## 四、计算题

1. **推导线性回归模型的损失函数梯度与梯度下降更新公式。**
	 假设线性回归的损失函数为均方误差：
	 J(θ)=12m∑i=1m(hθ(x(i))−y(i))2,hθ(x)=θTx.J(\theta) = \frac{1}{2m} \sum_{i=1}^m (h_\theta(x^{(i)}) - y^{(i)})^2, \quad h_\theta(x) = \theta^T x.
	 求$J(\theta)$对参数$\theta$的偏导并写出梯度下降的更新公式。

	【答案】
	 首先，求损失函数对参数$\theta$的梯度：
	 ∂J∂θ=1mXT(Xθ−y),\frac{\partial J}{\partial \theta} = \frac{1}{m} X^T (X\theta - y),
	 其中$X$为样本矩阵，$y$为标签向量。梯度下降更新公式为：
	 θ:=θ−α1mXT(Xθ−y),\theta := \theta - \alpha \frac{1}{m} X^T (X\theta - y),
	 其中$\alpha$为学习率。或者分量形式：
	 θj:=θj−α(1m∑i=1m(hθ(x(i))−y(i))xj(i)).\theta_j := \theta_j - \alpha \left(\frac{1}{m} \sum_{i=1}^m (h_\theta(x^{(i)}) - y^{(i)}) x_j^{(i)}\right).

2. **决策树划分指标计算。**
	 某数据集共有8个样本，其类别正负分布如下：正例4个、负例4个。考虑特征$A$取值为0或1的划分，其中当$A=0$时，有3个正例、1个负例；当$A=1$时，有1个正例、3个负例。请计算划分前的数据集信息熵$H(Y)$、划分后的条件熵$H(Y|A)$以及特征$A$的信息增益$IG$。

	【答案】
	 划分前类别分布为正例4、负例4，总体熵：
	 H(Y)=−48log⁡248−48log⁡248=1.0.H(Y) = -\frac{4}{8}\log_2\frac{4}{8} - \frac{4}{8}\log_2\frac{4}{8} = 1.0.
	 划分后：$A=0$子集有4个样本（3正1负），熵为
	 H(Y∣A=0)=−34log⁡234−14log⁡214≈0.811.H(Y|A=0) = -\frac{3}{4}\log_2\frac{3}{4} - \frac{1}{4}\log_2\frac{1}{4} \approx 0.811.
	 $A=1$子集有4个样本（1正3负），熵为
	 H(Y∣A=1)=−14log⁡214−34log⁡234≈0.811.H(Y|A=1) = -\frac{1}{4}\log_2\frac{1}{4} - \frac{3}{4}\log_2\frac{3}{4} \approx 0.811.
	 加权条件熵：
	 H(Y∣A)=48H(Y∣A=0)+48H(Y∣A=1)=0.811.H(Y|A) = \frac{4}{8} H(Y|A=0) + \frac{4}{8} H(Y|A=1) = 0.811.
	 信息增益：
	 IG(A)=H(Y)−H(Y∣A)=1.0−0.811=0.189.IG(A) = H(Y) - H(Y|A) = 1.0 - 0.811 = 0.189.

3. **对数几率回归计算。**
	 假设一个简单的对数几率回归模型，参数为$\theta = 1$，偏置$b=0$，输入特征$x=1$。对于真实标签$y=0$，计算模型的预测概率$h_\theta(x)$（采用Sigmoid函数）和对数损失：
	 L=−[yln⁡(hθ(x))+(1−y)ln⁡(1−hθ(x))].L = -[y\ln(h_\theta(x)) + (1-y)\ln(1-h_\theta(x))].

	【答案】
	 模型输出概率为Sigmoid函数：
	 hθ(x)=σ(θx+b)=11+e−1⋅1≈0.7311.h_\theta(x) = \sigma(\theta x + b) = \frac{1}{1+e^{-1\cdot1}} \approx 0.7311.
	 对数损失为：
	 L=−[0⋅ln⁡(0.7311)+1⋅ln⁡(1−0.7311)]=−ln⁡(0.2689)≈1.3133.L = -[0\cdot\ln(0.7311) + 1\cdot\ln(1 - 0.7311)] = -\ln(0.2689) \approx 1.3133.

4. **AdaBoost权重更新。**
	 假设在AdaBoost算法中，有3个样本，初始权重均为$1/3$。在某一轮中，弱分类器的错误率为$e=1/3$，其中仅样本3被错误分类。计算该弱分类器的权重$\alpha$，并说明更新后每个样本的新权重（归一化后）。

	【答案】
	 弱分类器权重计算：
	 α=12ln⁡1−ee=12ln⁡2/31/3=12ln⁡2≈0.3466.\alpha = \frac{1}{2}\ln\frac{1-e}{e} = \frac{1}{2}\ln\frac{2/3}{1/3} = \frac{1}{2}\ln 2 \approx 0.3466.
	 错误样本（样本3）权重更新前为$w_3 = 1/3$，更新后乘以$e^{\alpha}$：
	 w3′=1/3×e0.3466Z.w'_3 = \frac{1/3 \times e^{0.3466}}{Z}.
	 正确分类的样本（样本1和样本2）权重更新前为$1/3$，更新后乘以$e^{-\alpha}$：
	 $w1′=w2′=1/3×e−0.3466Z.w'_1 = w'_2 = \frac{1/3 \times e^{-0.3466}}{Z}$.
	 其中$Z$为归一化常数，使权重之和为1。计算得到：更新前的未归一化权重分别为$0.4714$（样本3）和$0.2357$（样本1、2），归一化后样本3权重约为0.5，样本1和2各约为0.25。